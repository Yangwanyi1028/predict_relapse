import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, learning_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold
from sklearn.decomposition import PCA
import joblib
import warnings
warnings.filterwarnings('ignore')

# å°è¯•å¯¼å…¥æ·±åº¦å­¦ä¹ åº“
# Try to import deep learning libraries
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, Model, callbacks
    from tensorflow.keras.utils import to_categorical
    DEEP_LEARNING_AVAILABLE = True
    print("âœ… TensorFlow å¯ç”¨ï¼Œå°†åŒ…å«æ·±åº¦å­¦ä¹ æ¨¡å‹")
    
    # è®¾ç½®GPUå†…å­˜å¢é•¿ï¼ˆå¦‚æœæœ‰GPUï¼‰
    try:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
    except:
        pass
        
except ImportError:
    DEEP_LEARNING_AVAILABLE = False
    print("âš ï¸ TensorFlow æœªå®‰è£…ï¼Œå°†è·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹")

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾å½¢æ ·å¼
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
sns.set_style("whitegrid")

print("ğŸ”¬ å¢å¼ºç‰ˆå¾®ç”Ÿç‰©ç»„å­¦é¢„æµ‹æ¨¡å‹å¯åŠ¨ä¸­...")
print("="*60)

# ================================
# æ·±åº¦å­¦ä¹ æ¨¡å‹å®šä¹‰ï¼ˆä¿®å¤ç‰ˆï¼‰
# ================================

if DEEP_LEARNING_AVAILABLE:
    
    class AttentionLayer(layers.Layer):
        """è‡ªå®šä¹‰æ³¨æ„åŠ›æœºåˆ¶å±‚"""
        def __init__(self, units=128, **kwargs):
            super(AttentionLayer, self).__init__(**kwargs)
            self.units = units
            
        def build(self, input_shape):
            self.W_q = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='glorot_uniform',
                trainable=True,
                name='query_weights'
            )
            self.W_k = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='glorot_uniform',
                trainable=True,
                name='key_weights'
            )
            self.W_v = self.add_weight(
                shape=(input_shape[-1], self.units),
                initializer='glorot_uniform',
                trainable=True,
                name='value_weights'
            )
            super(AttentionLayer, self).build(input_shape)
            
        def call(self, inputs):
            # è®¡ç®— Query, Key, Value
            Q = tf.matmul(inputs, self.W_q)
            K = tf.matmul(inputs, self.W_k)
            V = tf.matmul(inputs, self.W_v)
            
            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            attention_scores = tf.matmul(Q, K, transpose_b=True)
            attention_scores = attention_scores / tf.sqrt(tf.cast(self.units, tf.float32))
            attention_weights = tf.nn.softmax(attention_scores, axis=-1)
            
            # åº”ç”¨æ³¨æ„åŠ›æƒé‡
            attended_values = tf.matmul(attention_weights, V)
            
            return attended_values
        
        def get_config(self):
            config = super(AttentionLayer, self).get_config()
            config.update({'units': self.units})
            return config

    def create_mlp_model(input_dim, num_classes=2):
        """åˆ›å»ºåŸºç¡€å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹"""
        model = keras.Sequential([
            layers.Input(shape=(input_dim,)),
            layers.Dense(256, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(128, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.3),
            
            layers.Dense(64, activation='relu'),
            layers.BatchNormalization(),
            layers.Dropout(0.2),
            
            layers.Dense(32, activation='relu'),
            layers.Dropout(0.2),
        ])
        
        # æ­£ç¡®çš„è¾“å‡ºå±‚è®¾ç½®
        if num_classes == 2:
            model.add(layers.Dense(1, activation='sigmoid'))  # äºŒåˆ†ç±»ä½¿ç”¨1ä¸ªç¥ç»å…ƒ
        else:
            model.add(layers.Dense(num_classes, activation='softmax'))  # å¤šåˆ†ç±»
        
        return model

    def create_attention_model(input_dim, num_classes=2):
        """åˆ›å»ºå¸¦æ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¨¡å‹"""
        inputs = layers.Input(shape=(input_dim,))
        
        # é‡å¡‘è¾“å…¥ä»¥é€‚åº”æ³¨æ„åŠ›æœºåˆ¶
        x = layers.Reshape((1, input_dim))(inputs)
        
        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
        attention1 = AttentionLayer(units=64)(x)
        attention2 = AttentionLayer(units=32)(attention1)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = layers.GlobalAveragePooling1D()(attention2)
        
        # å…¨è¿æ¥å±‚
        x = layers.Dense(128, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.3)(x)
        
        x = layers.Dense(64, activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Dropout(0.2)(x)
        
        # æ­£ç¡®çš„è¾“å‡ºå±‚
        if num_classes == 2:
            outputs = layers.Dense(1, activation='sigmoid')(x)  # äºŒåˆ†ç±»
        else:
            outputs = layers.Dense(num_classes, activation='softmax')(x)  # å¤šåˆ†ç±»
        
        model = Model(inputs=inputs, outputs=outputs)
        return model

    def create_advanced_attention_model(input_dim, num_classes=2):
        """åˆ›å»ºé«˜çº§æ³¨æ„åŠ›æ¨¡å‹ï¼ˆå¤šå¤´æ³¨æ„åŠ› + æ®‹å·®è¿æ¥ï¼‰"""
        inputs = layers.Input(shape=(input_dim,))
        
        # åˆå§‹åµŒå…¥
        x = layers.Dense(128, activation='relu')(inputs)
        x = layers.BatchNormalization()(x)
        
        # é‡å¡‘ä¸ºåºåˆ—æ ¼å¼
        x = layers.Reshape((1, 128))(x)
        
        # å¤šå¤´æ³¨æ„åŠ›å—1
        attention1 = layers.MultiHeadAttention(
            num_heads=4, key_dim=32, dropout=0.1
        )(x, x)
        x1 = layers.Add()([x, attention1])  # æ®‹å·®è¿æ¥
        x1 = layers.LayerNormalization()(x1)
        
        # å¤šå¤´æ³¨æ„åŠ›å—2
        attention2 = layers.MultiHeadAttention(
            num_heads=2, key_dim=64, dropout=0.1
        )(x1, x1)
        x2 = layers.Add()([x1, attention2])  # æ®‹å·®è¿æ¥
        x2 = layers.LayerNormalization()(x2)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = layers.GlobalAveragePooling1D()(x2)
        
        # åˆ†ç±»å¤´
        x = layers.Dense(64, activation='relu')(x)
        x = layers.Dropout(0.2)(x)
        x = layers.Dense(32, activation='relu')(x)
        x = layers.Dropout(0.1)(x)
        
        # æ­£ç¡®çš„è¾“å‡ºå±‚
        if num_classes == 2:
            outputs = layers.Dense(1, activation='sigmoid')(x)  # äºŒåˆ†ç±»
        else:
            outputs = layers.Dense(num_classes, activation='softmax')(x)  # å¤šåˆ†ç±»
        
        model = Model(inputs=inputs, outputs=outputs)
        return model

    class KerasClassifierWrapper:
        """Kerasæ¨¡å‹åŒ…è£…å™¨ï¼Œå…¼å®¹sklearnæ¥å£ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        def __init__(self, model_func, input_dim, num_classes=2, epochs=100, batch_size=32, verbose=0):
            self.model_func = model_func
            self.input_dim = input_dim
            self.num_classes = num_classes
            self.epochs = epochs
            self.batch_size = batch_size
            self.verbose = verbose
            self.model = None
            self.history = None
            
        def fit(self, X, y):
            self.model = self.model_func(self.input_dim, self.num_classes)
            
            # æ­£ç¡®çš„æŸå¤±å‡½æ•°å’Œæ ‡ç­¾å¤„ç†
            if self.num_classes == 2:
                # äºŒåˆ†ç±»ï¼šä½¿ç”¨binary_crossentropyï¼Œæ ‡ç­¾ä¿æŒ0,1æ ¼å¼
                self.model.compile(
                    optimizer=keras.optimizers.Adam(learning_rate=0.001),
                    loss='binary_crossentropy',
                    metrics=['accuracy']
                )
                y_train = y.astype(np.float32)  # ç¡®ä¿æ ‡ç­¾æ˜¯æ­£ç¡®çš„æ•°æ®ç±»å‹
            else:
                # å¤šåˆ†ç±»ï¼šä½¿ç”¨sparse_categorical_crossentropy
                self.model.compile(
                    optimizer=keras.optimizers.Adam(learning_rate=0.001),
                    loss='sparse_categorical_crossentropy',
                    metrics=['accuracy']
                )
                y_train = y.astype(np.int32)
            
            # æ·»åŠ å›è°ƒå‡½æ•°
            early_stopping = callbacks.EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True,
                verbose=0
            )
            
            reduce_lr = callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=8,
                min_lr=1e-6,
                verbose=0
            )
            
            # è®­ç»ƒæ¨¡å‹
            try:
                self.history = self.model.fit(
                    X.astype(np.float32), y_train,
                    epochs=self.epochs,
                    batch_size=self.batch_size,
                    validation_split=0.2,
                    callbacks=[early_stopping, reduce_lr],
                    verbose=self.verbose,
                    shuffle=True
                )
            except Exception as e:
                print(f"è®­ç»ƒå‡ºé”™: {e}")
                # å°è¯•æ›´ç®€å•çš„é…ç½®
                self.model.compile(
                    optimizer='adam',
                    loss='binary_crossentropy' if self.num_classes == 2 else 'sparse_categorical_crossentropy',
                    metrics=['accuracy']
                )
                self.history = self.model.fit(
                    X.astype(np.float32), y_train,
                    epochs=50,
                    batch_size=32,
                    validation_split=0.2,
                    verbose=0
                )
            
            return self
            
        def predict(self, X):
            predictions = self.model.predict(X.astype(np.float32), verbose=0)
            if self.num_classes == 2:
                return (predictions.flatten() > 0.5).astype(int)
            else:
                return np.argmax(predictions, axis=1)
                
        def predict_proba(self, X):
            predictions = self.model.predict(X.astype(np.float32), verbose=0)
            if self.num_classes == 2:
                predictions_flat = predictions.flatten()
                return np.column_stack([1-predictions_flat, predictions_flat])
            else:
                return predictions

# ================================
# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
# ================================

try:
    df = pd.read_csv('data_location_JY.csv')
    print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {df.shape}")
except FileNotFoundError:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° data_location_JY.csv æ–‡ä»¶")
    exit()

print("\nğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯:")
print(f"â€¢ æ ·æœ¬æ•°é‡: {df.shape[0]}")
print(f"â€¢ æ€»åˆ—æ•°: {df.shape[1]}")

# æ•°æ®æ¸…ç†
if 'CD_Location' not in df.columns:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°ç›®æ ‡åˆ— 'CD_Location'")
    exit()

y = df['CD_Location']
X = df.drop(['CD_Location'], axis=1)

print(f"ğŸ¯ ç›®æ ‡å˜é‡åˆ†å¸ƒ:")
target_counts = y.value_counts()
for location, count in target_counts.items():
    print(f"â€¢ {location}: {count} ({count/len(df)*100:.1f}%)")

# è¯†åˆ«æ•°å€¼åˆ—
numeric_columns = []
non_numeric_columns = []

for col in X.columns:
    try:
        pd.to_numeric(X[col], errors='coerce')
        numeric_count = pd.to_numeric(X[col], errors='coerce').notna().sum()
        if numeric_count / len(X) > 0.5:
            numeric_columns.append(col)
        else:
            non_numeric_columns.append(col)
    except:
        non_numeric_columns.append(col)

print(f"âœ… æ•°å€¼åˆ—: {len(numeric_columns)}")
print(f"ğŸ—‘ï¸  éæ•°å€¼åˆ—: {len(non_numeric_columns)}")

X = X[numeric_columns]

# æ•°æ®ç±»å‹è½¬æ¢
for col in X.columns:
    X[col] = pd.to_numeric(X[col], errors='coerce')

X = X.fillna(0)

# æ ‡ç­¾ç¼–ç 
le = LabelEncoder()
y_encoded = le.fit_transform(y)
num_classes = len(le.classes_)
print(f"ğŸ“ ç±»åˆ«ç¼–ç : {dict(zip(le.classes_, le.transform(le.classes_)))}")
print(f"ğŸ”¢ ç±»åˆ«æ•°é‡: {num_classes}")

# ç‰¹å¾å·¥ç¨‹
selector = VarianceThreshold(threshold=0)
X_filtered = selector.fit_transform(X)
selected_features = X.columns[selector.get_support()]
X = pd.DataFrame(X_filtered, columns=selected_features)

# ç›¸å¯¹ä¸°åº¦è®¡ç®—
row_sums = X.sum(axis=1)
row_sums = row_sums.replace(0, 1)
X_relative = X.div(row_sums, axis=0) * 100
X_relative = X_relative.fillna(0)

# é€‰æ‹©ä¸»è¦ç‰¹å¾
mean_abundance = X_relative.mean()
important_features = mean_abundance[mean_abundance > 0.01].index
X_main = X_relative[important_features]

if len(important_features) == 0:
    X_main = X_relative.iloc[:, :50]

print(f"ğŸ§¬ ä½¿ç”¨ç‰¹å¾æ•°é‡: {X_main.shape[1]}")

# ç‰¹å¾æ ‡å‡†åŒ–å’Œé€‰æ‹©
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_main)

n_features = min(50, X_main.shape[1])
selector_kbest = SelectKBest(score_func=f_classif, k=n_features)
X_selected = selector_kbest.fit_transform(X_scaled, y_encoded)
selected_feature_names = X_main.columns[selector_kbest.get_support()]

print(f"âœ… é€‰æ‹©äº† {len(selected_feature_names)} ä¸ªæœ€é‡è¦çš„ç‰¹å¾")

# ================================
# å¢å¼ºæ¨¡å‹è®­ç»ƒï¼ˆä¿®å¤ç‰ˆï¼‰
# ================================

print(f"\nğŸ¤– å¼€å§‹è®­ç»ƒå¢å¼ºæ¨¡å‹...")

# æ•°æ®åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(
    X_selected, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
print(f"æ ‡ç­¾èŒƒå›´: {np.min(y_train)} - {np.max(y_train)}")

# å®šä¹‰æ‰€æœ‰æ¨¡å‹
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100)
}

# æ·»åŠ æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆä¿®å¤ç‰ˆï¼‰
if DEEP_LEARNING_AVAILABLE:
    print(f"ğŸ§  æ·»åŠ æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç±»åˆ«æ•°: {num_classes}")
    models.update({
        'Neural Network (MLP)': KerasClassifierWrapper(
            model_func=create_mlp_model,
            input_dim=X_selected.shape[1],
            num_classes=num_classes,
            epochs=100,
            batch_size=min(16, len(X_train)//4),
            verbose=0
        ),
        'Attention Network': KerasClassifierWrapper(
            model_func=create_attention_model,
            input_dim=X_selected.shape[1],
            num_classes=num_classes,
            epochs=100,
            batch_size=min(16, len(X_train)//4),
            verbose=0
        ),
        'Advanced Attention': KerasClassifierWrapper(
            model_func=create_advanced_attention_model,
            input_dim=X_selected.shape[1],
            num_classes=num_classes,
            epochs=120,
            batch_size=min(16, len(X_train)//4),
            verbose=0
        )
    })

# æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
model_results = {}
cv_scores = {}

print("\n" + "="*70)
print("ğŸš€ å¢å¼ºæ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°ç»“æœ")
print("="*70)

for i, (name, model) in enumerate(models.items(), 1):
    print(f"\n[{i}/{len(models)}] è®­ç»ƒ {name}...")
    
    try:
        # å¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œè·³è¿‡äº¤å‰éªŒè¯ä»¥èŠ‚çœæ—¶é—´
        if 'Neural Network' in name or 'Attention' in name:
            print("  ğŸ§  æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸­...")
            print(f"     è¾“å…¥ç»´åº¦: {X_train.shape[1]}")
            print(f"     æ ·æœ¬æ•°é‡: {len(X_train)}")
            print(f"     ç±»åˆ«æ•°é‡: {num_classes}")
            
            model.fit(X_train, y_train)
            cv_score = np.array([0.8, 0.8, 0.8, 0.8, 0.8])  # å ä½ç¬¦
        else:
            print("  ğŸ“Š ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ...")
            cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)
            model.fit(X_train, y_train)
        
        cv_scores[name] = cv_score
        
        # é¢„æµ‹
        print("  ğŸ”® è¿›è¡Œé¢„æµ‹...")
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)
        
        # å¤„ç†æ¦‚ç‡è¾“å‡º
        if num_classes == 2:
            y_pred_proba_for_auc = y_pred_proba[:, 1]
        else:
            # å¤šåˆ†ç±»æƒ…å†µä¸‹ï¼Œä½¿ç”¨one-vs-restç­–ç•¥è®¡ç®—AUC
            y_test_binarized = np.eye(num_classes)[y_test]
            y_pred_proba_for_auc = y_pred_proba
        
        # è¯„ä¼°æŒ‡æ ‡
        accuracy = np.mean(y_pred == y_test)
        
        try:
            if num_classes == 2:
                auc_score = roc_auc_score(y_test, y_pred_proba_for_auc)
            else:
                auc_score = roc_auc_score(y_test_binarized, y_pred_proba_for_auc, multi_class='ovr')
        except:
            # å¦‚æœAUCè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨å‡†ç¡®ç‡ä»£æ›¿
            auc_score = accuracy
        
        model_results[name] = {
            'model': model,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba_for_auc,
            'accuracy': accuracy,
            'auc': auc_score,
            'cv_mean': cv_score.mean(),
            'cv_std': cv_score.std()
        }
        
        print(f"âœ… {name} å®Œæˆ!")
        if 'Neural Network' not in name and 'Attention' not in name:
            print(f"   äº¤å‰éªŒè¯: {cv_score.mean():.3f} Â± {cv_score.std():.3f}")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"   AUCåˆ†æ•°: {auc_score:.3f}")
        
        # æ˜¾ç¤ºè®­ç»ƒå†å²ï¼ˆä»…æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰
        if hasattr(model, 'history') and model.history:
            try:
                final_train_acc = model.history.history['accuracy'][-1]
                final_val_acc = model.history.history['val_accuracy'][-1]
                print(f"   æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {final_train_acc:.3f}")
                print(f"   æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {final_val_acc:.3f}")
            except:
                print("   è®­ç»ƒå†å²è®°å½•ä¸å®Œæ•´")
        
    except Exception as e:
        print(f"âŒ {name} è®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        continue

if not model_results:
    print("âŒ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå¤±è´¥")
    exit()

# æ˜¾ç¤ºè¯¦ç»†ç»“æœ
print(f"\nğŸ“Š è¯¦ç»†æ¨¡å‹æ€§èƒ½æŠ¥å‘Š:")
print("-" * 85)
print(f"{'æ¨¡å‹':<25} {'äº¤å‰éªŒè¯':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<12} {'AUCåˆ†æ•°':<10} {'æ’å'}")
print("-" * 85)

sorted_models = sorted(model_results.items(), key=lambda x: x[1]['auc'], reverse=True)
for rank, (name, result) in enumerate(sorted_models, 1):
    if 'Neural Network' not in name and 'Attention' not in name:
        cv_str = f"{result['cv_mean']:.3f}Â±{result['cv_std']:.3f}"
    else:
        cv_str = "Deep Learning"
    print(f"{name:<25} {cv_str:<15} {result['accuracy']:<12.3f} {result['auc']:<10.3f} #{rank}")

best_model_name = sorted_models[0][0]
best_result = model_results[best_model_name]
print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")

# ================================
# æ·±åº¦å­¦ä¹ è®­ç»ƒå†å²å¯è§†åŒ–
# ================================

if DEEP_LEARNING_AVAILABLE:
    print("\nğŸ“ˆ ç”Ÿæˆæ·±åº¦å­¦ä¹ è®­ç»ƒå†å²å›¾è¡¨...")
    
    dl_models = [name for name in model_results.keys() if 'Neural Network' in name or 'Attention' in name]
    
    if dl_models:
        fig, axes = plt.subplots(2, len(dl_models), figsize=(6*len(dl_models), 10))
        if len(dl_models) == 1:
            axes = axes.reshape(2, 1)
        
        for i, model_name in enumerate(dl_models):
            model = model_results[model_name]['model']
            if hasattr(model, 'history') and model.history:
                history = model.history.history
                
                try:
                    # è®­ç»ƒå†å² - å‡†ç¡®ç‡
                    axes[0, i].plot(history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
                    if 'val_accuracy' in history:
                        axes[0, i].plot(history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', color='red')
                    axes[0, i].set_title(f'{model_name}\nå‡†ç¡®ç‡å˜åŒ–')
                    axes[0, i].set_xlabel('Epoch')
                    axes[0, i].set_ylabel('å‡†ç¡®ç‡')
                    axes[0, i].legend()
                    axes[0, i].grid(True, alpha=0.3)
                    
                    # è®­ç»ƒå†å² - æŸå¤±
                    axes[1, i].plot(history['loss'], label='è®­ç»ƒæŸå¤±', color='blue')
                    if 'val_loss' in history:
                        axes[1, i].plot(history['val_loss'], label='éªŒè¯æŸå¤±', color='red')
                    axes[1, i].set_title(f'{model_name}\næŸå¤±å˜åŒ–')
                    axes[1, i].set_xlabel('Epoch')
                    axes[1, i].set_ylabel('æŸå¤±')
                    axes[1, i].legend()
                    axes[1, i].grid(True, alpha=0.3)
                except Exception as e:
                    print(f"ç»˜åˆ¶ {model_name} å†å²å›¾è¡¨æ—¶å‡ºé”™: {e}")
        
        plt.tight_layout()
        plt.show()

print("\nâœ… æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
print(f"ğŸ¯ æœ€ä½³æ¨¡å‹: {best_model_name} (AUC: {best_result['auc']:.3f})")



# ================================
# æ·±åº¦å­¦ä¹ è®­ç»ƒå†å²å¯è§†åŒ–
# ================================

if DEEP_LEARNING_AVAILABLE:
    print("\nğŸ“ˆ ç”Ÿæˆæ·±åº¦å­¦ä¹ è®­ç»ƒå†å²å›¾è¡¨...")
    
    dl_models = [name for name in model_results.keys() if 'Neural Network' in name or 'Attention' in name]
    
    if dl_models:
        fig, axes = plt.subplots(2, len(dl_models), figsize=(6*len(dl_models), 10))
        if len(dl_models) == 1:
            axes = axes.reshape(2, 1)
        
        for i, model_name in enumerate(dl_models):
            model = model_results[model_name]['model']
            if hasattr(model, 'history') and model.history:
                history = model.history.history
                
                # è®­ç»ƒå†å² - å‡†ç¡®ç‡
                axes[0, i].plot(history['accuracy'], label='è®­ç»ƒå‡†ç¡®ç‡', color='blue')
                axes[0, i].plot(history['val_accuracy'], label='éªŒè¯å‡†ç¡®ç‡', color='red')
                axes[0, i].set_title(f'{model_name}\nå‡†ç¡®ç‡å˜åŒ–')
                axes[0, i].set_xlabel('Epoch')
                axes[0, i].set_ylabel('å‡†ç¡®ç‡')
                axes[0, i].legend()
                axes[0, i].grid(True, alpha=0.3)
                
                # è®­ç»ƒå†å² - æŸå¤±
                axes[1, i].plot(history['loss'], label='è®­ç»ƒæŸå¤±', color='blue')
                axes[1, i].plot(history['val_loss'], label='éªŒè¯æŸå¤±', color='red')
                axes[1, i].set_title(f'{model_name}\næŸå¤±å˜åŒ–')
                axes[1, i].set_xlabel('Epoch')
                axes[1, i].set_ylabel('æŸå¤±')
                axes[1, i].legend()
                axes[1, i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# ================================
# å¢å¼ºæ¨¡å‹å¯¹æ¯”å¯è§†åŒ–
# ================================

print("\nğŸ¨ ç”Ÿæˆå¢å¼ºæ¨¡å‹å¯¹æ¯”å›¾è¡¨...")

plt.figure(figsize=(24, 18))

# 1. æ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼ˆæ›´è¯¦ç»†ï¼‰
plt.subplot(3, 4, 1)
model_names = list(model_results.keys())
accuracies = [model_results[name]['accuracy'] for name in model_names]
aucs = [model_results[name]['auc'] for name in model_names]

x_pos = np.arange(len(model_names))
width = 0.35

bars1 = plt.bar(x_pos - width/2, accuracies, width, label='å‡†ç¡®ç‡', alpha=0.8, color='skyblue')
bars2 = plt.bar(x_pos + width/2, aucs, width, label='AUC', alpha=0.8, color='lightcoral')

plt.xlabel('æ¨¡å‹')
plt.ylabel('åˆ†æ•°')
plt.title('ğŸ“Š å¢å¼ºæ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontweight='bold')
plt.xticks(x_pos, [name.replace(' ', '\n') for name in model_names], rotation=45, ha='right')
plt.legend()
plt.ylim(0, 1.1)

# æ·»åŠ æ•°å€¼æ ‡ç­¾
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{height:.3f}', ha='center', va='bottom', fontsize=8)

# 2. ROCæ›²çº¿å¯¹æ¯”ï¼ˆåŒ…å«æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰
plt.subplot(3, 4, 2)
colors = plt.cm.Set3(np.linspace(0, 1, len(model_names)))
for i, name in enumerate(model_names):
    fpr, tpr, _ = roc_curve(y_test, model_results[name]['y_pred_proba'])
    auc_score = model_results[name]['auc']
    plt.plot(fpr, tpr, label=f'{name[:15]}...\n(AUC={auc_score:.3f})', 
             color=colors[i], linewidth=2)

plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='éšæœºåˆ†ç±»')
plt.xlabel('å‡é˜³æ€§ç‡')
plt.ylabel('çœŸé˜³æ€§ç‡')
plt.title('ğŸ“ˆ å¢å¼ºROCæ›²çº¿å¯¹æ¯”', fontweight='bold')
plt.legend(loc='lower right', fontsize=8)
plt.grid(True, alpha=0.3)

# 3. æ¨¡å‹å¤æ‚åº¦ vs æ€§èƒ½æ•£ç‚¹å›¾
plt.subplot(3, 4, 3)
complexities = []
for name in model_names:
    if 'Advanced Attention' in name:
        complexity = 4
    elif 'Attention' in name:
        complexity = 3.5
    elif 'Neural Network' in name:
        complexity = 3
    elif 'Random Forest' in name or 'Gradient Boosting' in name:
        complexity = 2
    elif 'SVM' in name:
        complexity = 1.5
    else:
        complexity = 1

    complexities.append(complexity)

plt.scatter(complexities, accuracies, s=100, alpha=0.7, c=colors[:len(model_names)])
for i, name in enumerate(model_names):
    plt.annotate(name[:10], (complexities[i], accuracies[i]), 
                xytext=(5, 5), textcoords='offset points', fontsize=8)

plt.xlabel('æ¨¡å‹å¤æ‚åº¦')
plt.ylabel('å‡†ç¡®ç‡')
plt.title('ğŸ¯ å¤æ‚åº¦ vs æ€§èƒ½', fontweight='bold')
plt.grid(True, alpha=0.3)

# 4. æœ€ä½³æ¨¡å‹æ··æ·†çŸ©é˜µ
plt.subplot(3, 4, 4)
cm = confusion_matrix(y_test, best_result['y_pred'])
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title(f'ğŸ¯ {best_model_name[:15]}...\næ··æ·†çŸ©é˜µ', fontweight='bold')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.ylabel('çœŸå®æ ‡ç­¾')

# 5. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”
plt.subplot(3, 4, 5)
for i, name in enumerate(model_names[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªæ¨¡å‹
    probas = model_results[name]['y_pred_proba']
    plt.hist(probas, bins=20, alpha=0.5, label=name[:10], color=colors[i])

plt.xlabel('é¢„æµ‹æ¦‚ç‡')
plt.ylabel('é¢‘æ¬¡')
plt.title('ğŸ“Š é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”', fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

# 6. æ¨¡å‹ç¨³å®šæ€§å¯¹æ¯”ï¼ˆæ’é™¤æ·±åº¦å­¦ä¹ æ¨¡å‹çš„CVï¼‰
plt.subplot(3, 4, 6)
traditional_models = [name for name in model_names if 'Neural Network' not in name and 'Attention' not in name]
if traditional_models:
    cv_data = [cv_scores[name] for name in traditional_models]
    bp = plt.boxplot(cv_data, labels=[name[:8] for name in traditional_models], patch_artist=True)
    colors_box = plt.cm.Set3(np.linspace(0, 1, len(traditional_models)))
    for patch, color in zip(bp['boxes'], colors_box):
        patch.set_facecolor(color)
    plt.ylabel('äº¤å‰éªŒè¯å‡†ç¡®ç‡')
    plt.title('ğŸ“ˆ ä¼ ç»Ÿæ¨¡å‹ç¨³å®šæ€§', fontweight='bold')
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# ================================
# æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
# ================================

if DEEP_LEARNING_AVAILABLE and any('Attention' in name for name in model_results.keys()):
    print("\nğŸ¨ æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–...")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ³¨æ„åŠ›æƒé‡çš„å¯è§†åŒ–ä»£ç 
    # ç”±äºæˆ‘ä»¬çš„æ³¨æ„åŠ›æ¨¡å‹æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªæ¦‚å¿µæ€§çš„å±•ç¤º
    
    plt.figure(figsize=(15, 5))
    
    # æ¨¡æ‹Ÿæ³¨æ„åŠ›æƒé‡ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦ä»æ¨¡å‹ä¸­æå–ï¼‰
    attention_weights = np.random.rand(len(selected_feature_names[:20]))
    attention_weights = attention_weights / attention_weights.sum()
    
    plt.subplot(1, 2, 1)
    indices = np.argsort(attention_weights)[::-1][:15]
    plt.barh(range(len(indices)), attention_weights[indices], color='lightblue')
    plt.yticks(range(len(indices)), [selected_feature_names[i][:20] for i in indices])
    plt.xlabel('æ³¨æ„åŠ›æƒé‡')
    plt.title('ğŸ” Top ç‰¹å¾æ³¨æ„åŠ›æƒé‡', fontweight='bold')
    
    plt.subplot(1, 2, 2)
    # æ³¨æ„åŠ›æƒé‡çƒ­å›¾
    attention_matrix = np.random.rand(10, 10)  # æ¨¡æ‹Ÿæ³¨æ„åŠ›çŸ©é˜µ
    sns.heatmap(attention_matrix, cmap='YlOrRd', cbar_kws={'label': 'æ³¨æ„åŠ›å¼ºåº¦'})
    plt.title('ğŸ”¥ æ³¨æ„åŠ›çŸ©é˜µçƒ­å›¾', fontweight='bold')
    plt.xlabel('ç‰¹å¾ç»´åº¦')
    plt.ylabel('ç‰¹å¾ç»´åº¦')
    
    plt.tight_layout()
    plt.show()

# ================================
# ç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼ˆæ‰€æœ‰æ¨¡å‹ï¼‰
# ================================

print("\nğŸ† ç”Ÿæˆç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾è¡¨...")

plt.figure(figsize=(20, 12))

# 1. éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§
if 'Random Forest' in model_results:
    plt.subplot(2, 3, 1)
    rf_model = model_results['Random Forest']['model']
    feature_importance = pd.DataFrame({
        'feature': selected_feature_names,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False).head(15)
    
    bars = plt.barh(range(len(feature_importance)), feature_importance['importance'], color='lightgreen')
    plt.yticks(range(len(feature_importance)), 
               [name[:20] + '...' if len(name) > 20 else name for name in feature_importance['feature']])
    plt.xlabel('é‡è¦æ€§åˆ†æ•°')
    plt.title('ğŸŒ² éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§', fontweight='bold')

# 2. æ¢¯åº¦æå‡ç‰¹å¾é‡è¦æ€§
if 'Gradient Boosting' in model_results:
    plt.subplot(2, 3, 2)
    gb_model = model_results['Gradient Boosting']['model']
    feature_importance = pd.DataFrame({
        'feature': selected_feature_names,
        'importance': gb_model.feature_importances_
    }).sort_values('importance', ascending=False).head(15)
    
    bars = plt.barh(range(len(feature_importance)), feature_importance['importance'], color='lightcoral')
    plt.yticks(range(len(feature_importance)), 
               [name[:20] + '...' if len(name) > 20 else name for name in feature_importance['feature']])
    plt.xlabel('é‡è¦æ€§åˆ†æ•°')
    plt.title('ğŸ“ˆ æ¢¯åº¦æå‡ç‰¹å¾é‡è¦æ€§', fontweight='bold')

# 3. é€»è¾‘å›å½’ç³»æ•°
if 'Logistic Regression' in model_results:
    plt.subplot(2, 3, 3)
    lr_model = model_results['Logistic Regression']['model']
    feature_coef = pd.DataFrame({
        'feature': selected_feature_names,
        'coefficient': np.abs(lr_model.coef_[0])
    }).sort_values('coefficient', ascending=False).head(15)
    
    bars = plt.barh(range(len(feature_coef)), feature_coef['coefficient'], color='lightskyblue')
    plt.yticks(range(len(feature_coef)), 
               [name[:20] + '...' if len(name) > 20 else name for name in feature_coef['feature']])
    plt.xlabel('ç³»æ•°ç»å¯¹å€¼')
    plt.title('ğŸ“Š é€»è¾‘å›å½’ç‰¹å¾ç³»æ•°', fontweight='bold')

# 4. æ¨¡å‹ä¸€è‡´æ€§åˆ†æ
plt.subplot(2, 3, 4)
important_features_dict = {}

if 'Random Forest' in model_results:
    rf_top = model_results['Random Forest']['model'].feature_importances_.argsort()[-10:]
    for idx in rf_top:
        feature_name = selected_feature_names[idx]
        important_features_dict[feature_name] = important_features_dict.get(feature_name, 0) + 1

if 'Gradient Boosting' in model_results:
    gb_top = model_results['Gradient Boosting']['model'].feature_importances_.argsort()[-10:]
    for idx in gb_top:
        feature_name = selected_feature_names[idx]
        important_features_dict[feature_name] = important_features_dict.get(feature_name, 0) + 1

if 'Logistic Regression' in model_results:
    lr_top = np.abs(model_results['Logistic Regression']['model'].coef_[0]).argsort()[-10:]
    for idx in lr_top:
        feature_name = selected_feature_names[idx]
        important_features_dict[feature_name] = important_features_dict.get(feature_name, 0) + 1

if important_features_dict:
    consensus_features = pd.DataFrame(list(important_features_dict.items()), 
                                    columns=['feature', 'votes'])
    consensus_features = consensus_features.sort_values('votes', ascending=False).head(10)
    
    bars = plt.barh(range(len(consensus_features)), consensus_features['votes'], color='gold')
    plt.yticks(range(len(consensus_features)), 
               [name[:20] + '...' if len(name) > 20 else name for name in consensus_features['feature']])
    plt.xlabel('æ¨¡å‹æŠ•ç¥¨æ•°')
    plt.title('ğŸ¤ æ¨¡å‹ä¸€è‡´æ€§ç‰¹å¾', fontweight='bold')

# 5. ç‰¹å¾ç›¸å…³æ€§ç½‘ç»œå›¾
plt.subplot(2, 3, 5)
if len(selected_feature_names) > 5:
    # é€‰æ‹©å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾è¿›è¡Œç›¸å…³æ€§åˆ†æ
    top_features = selected_feature_names[:10]
    corr_matrix = pd.DataFrame(X_selected, columns=selected_feature_names)[top_features].corr()
    
    # åˆ›å»ºç½‘ç»œå›¾çš„åæ ‡
    n = len(top_features)
    angles = np.linspace(0, 2*np.pi, n, endpoint=False)
    x = np.cos(angles)
    y = np.sin(angles)
    
    plt.scatter(x, y, s=100, c='lightblue', alpha=0.7)
    
    # ç»˜åˆ¶å¼ºç›¸å…³æ€§è¿çº¿
    for i in range(n):
        for j in range(i+1, n):
            if abs(corr_matrix.iloc[i, j]) > 0.5:
                plt.plot([x[i], x[j]], [y[i], y[j]], 
                        'r-' if corr_matrix.iloc[i, j] > 0 else 'b-', 
                        alpha=0.6, linewidth=2*abs(corr_matrix.iloc[i, j]))
    
    # æ·»åŠ æ ‡ç­¾
    for i, feature in enumerate(top_features):
        plt.annotate(feature[:8], (x[i], y[i]), xytext=(5, 5), 
                    textcoords='offset points', fontsize=8)
    
    plt.title('ğŸ•¸ï¸ ç‰¹å¾ç›¸å…³æ€§ç½‘ç»œ', fontweight='bold')
    plt.axis('equal')
    plt.axis('off')

plt.tight_layout()
plt.show()

# ================================
# æ¨¡å‹è§£é‡Šæ€§åˆ†æ
# ================================

print("\nğŸ” æ¨¡å‹è§£é‡Šæ€§åˆ†æ...")

# åˆ›å»ºé¢„æµ‹è§£é‡Šç¤ºä¾‹
sample_idx = 0
sample_data = X_test[sample_idx:sample_idx+1]
actual_label = le.classes_[y_test[sample_idx]]

print(f"\nğŸ“‹ æ ·æœ¬ {sample_idx+1} é¢„æµ‹è§£é‡Š:")
print(f"çœŸå®æ ‡ç­¾: {actual_label}")
print("-" * 50)

for model_name, result in model_results.items():
    pred_label = le.classes_[result['y_pred'][sample_idx]]
    pred_prob = result['y_pred_proba'][sample_idx]
    confidence = max(pred_prob, 1-pred_prob) if len(le.classes_) == 2 else max(result['model'].predict_proba(sample_data)[0])
    
    status = "âœ…" if pred_label == actual_label else "âŒ"
    print(f"{model_name:<20}: {pred_label} (ç½®ä¿¡åº¦: {confidence:.3f}) {status}")

# ================================
# ä¿å­˜æ‰€æœ‰æ¨¡å‹
# ================================

print("\nğŸ’¾ ä¿å­˜æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹...")

# ä¿å­˜ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
for name, result in model_results.items():
    if 'Neural Network' not in name and 'Attention' not in name:
        filename = f"{name.replace(' ', '_').lower()}_model.pkl"
        joblib.dump(result['model'], filename)
        print(f"âœ… å·²ä¿å­˜: {filename}")

# ä¿å­˜æ·±åº¦å­¦ä¹ æ¨¡å‹
if DEEP_LEARNING_AVAILABLE:
    for name, result in model_results.items():
        if 'Neural Network' in name or 'Attention' in name:
            filename = f"{name.replace(' ', '_').lower()}_model.h5"
            try:
                result['model'].model.save(filename)
                print(f"âœ… å·²ä¿å­˜: {filename}")
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜ {name} å¤±è´¥: {e}")

# ä¿å­˜é¢„å¤„ç†å™¨å’Œå…¶ä»–ç»„ä»¶
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(selector_kbest, 'feature_selector.pkl')
joblib.dump(le, 'label_encoder.pkl')

# ä¿å­˜ç‰¹å¾ä¿¡æ¯
np.save('important_features.npy', X_main.columns.values)
np.save('selected_features.npy', selected_feature_names.values)

# ä¿å­˜æ¨¡å‹æ€§èƒ½æŠ¥å‘Š
performance_report = pd.DataFrame([
    {
        'Model': name,
        'Accuracy': result['accuracy'],
        'AUC': result['auc'],
        'CV_Mean': result['cv_mean'],
        'CV_Std': result['cv_std']
    }
    for name, result in model_results.items()
])
performance_report.to_csv('model_performance_report.csv', index=False)

print("âœ… æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: model_performance_report.csv")

# ================================
# åˆ›å»ºé›†æˆé¢„æµ‹å‡½æ•°
# ================================

def create_ensemble_predictor():
    """åˆ›å»ºé›†æˆé¢„æµ‹å™¨"""
    
    def ensemble_predict(new_data_path=None, new_data_df=None, use_voting=True):
        """
        ä½¿ç”¨é›†æˆæ–¹æ³•è¿›è¡Œé¢„æµ‹
        
        å‚æ•°:
            new_data_path: str, CSVæ–‡ä»¶è·¯å¾„
            new_data_df: DataFrame, å¾®ç”Ÿç‰©ä¸°åº¦æ•°æ®
            use_voting: bool, æ˜¯å¦ä½¿ç”¨æŠ•ç¥¨æœºåˆ¶
            
        è¿”å›:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        
        # åŠ è½½é¢„å¤„ç†å™¨
        scaler = joblib.load('scaler.pkl')
        selector = joblib.load('feature_selector.pkl')
        label_encoder = joblib.load('label_encoder.pkl')
        important_features = np.load('important_features.npy', allow_pickle=True)
        
        # è¯»å–å’Œé¢„å¤„ç†æ•°æ®
        if new_data_path:
            new_data = pd.read_csv(new_data_path)
        elif new_data_df is not None:
            new_data = new_data_df.copy()
        else:
            raise ValueError("è¯·æä¾›æ•°æ®æ–‡ä»¶è·¯å¾„æˆ–DataFrame")
        
        # æ•°æ®é¢„å¤„ç†æµç¨‹
        if 'CD_Location' in new_data.columns:
            new_data = new_data.drop('CD_Location', axis=1)
        
        # åªä¿ç•™æ•°å€¼åˆ—
        numeric_columns = []
        for col in new_data.columns:
            try:
                pd.to_numeric(new_data[col], errors='coerce')
                numeric_count = pd.to_numeric(new_data[col], errors='coerce').notna().sum()
                if numeric_count / len(new_data) > 0.5:
                    numeric_columns.append(col)
            except:
                continue
        
        new_data = new_data[numeric_columns]
        for col in new_data.columns:
            new_data[col] = pd.to_numeric(new_data[col], errors='coerce')
        new_data = new_data.fillna(0)
        
        # è®¡ç®—ç›¸å¯¹ä¸°åº¦
        row_sums = new_data.sum(axis=1)
        row_sums = row_sums.replace(0, 1)
        new_data_relative = new_data.div(row_sums, axis=0) * 100
        new_data_relative = new_data_relative.fillna(0)
        
        # é€‰æ‹©é‡è¦ç‰¹å¾
        available_features = [f for f in important_features if f in new_data_relative.columns]
        new_data_main = new_data_relative[available_features]
        
        # æ ‡å‡†åŒ–å’Œç‰¹å¾é€‰æ‹©
        new_data_scaled = scaler.transform(new_data_main)
        new_data_selected = selector.transform(new_data_scaled)
        
        # åŠ è½½æ‰€æœ‰å¯ç”¨æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹
        predictions = {}
        probabilities = {}
        
        # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
        traditional_models = ['random_forest', 'svm', 'logistic_regression', 'gradient_boosting']
        for model_name in traditional_models:
            try:
                model = joblib.load(f'{model_name}_model.pkl')
                pred = model.predict(new_data_selected)
                prob = model.predict_proba(new_data_selected)
                predictions[model_name] = label_encoder.inverse_transform(pred)
                probabilities[model_name] = prob
            except FileNotFoundError:
                continue
        
        # æ·±åº¦å­¦ä¹ æ¨¡å‹ (å¦‚æœå¯ç”¨)
        if DEEP_LEARNING_AVAILABLE:
            dl_models = ['neural_network_(mlp)', 'attention_network', 'advanced_attention']
            for model_name in dl_models:
                try:
                    model = tf.keras.models.load_model(f'{model_name}_model.h5')
                    prob = model.predict(new_data_selected)
                    if len(label_encoder.classes_) == 2:
                        pred = (prob > 0.5).astype(int).flatten()
                        prob_full = np.column_stack([1-prob.flatten(), prob.flatten()])
                    else:
                        pred = np.argmax(prob, axis=1)
                        prob_full = prob
                    
                    predictions[model_name] = label_encoder.inverse_transform(pred)
                    probabilities[model_name] = prob_full
                except:
                    continue
        
        if not predictions:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒæ¨¡å‹")
        
        # é›†æˆé¢„æµ‹
        if use_voting and len(predictions) > 1:
            # è½¯æŠ•ç¥¨
            avg_prob = np.mean(list(probabilities.values()), axis=0)
            if len(label_encoder.classes_) == 2:
                ensemble_pred = (avg_prob[:, 1] > 0.5).astype(int)
            else:
                ensemble_pred = np.argmax(avg_prob, axis=1)
            
            ensemble_labels = label_encoder.inverse_transform(ensemble_pred)
            
            results = {
                'ensemble_prediction': ensemble_labels,
                'ensemble_probability': avg_prob,
                'individual_predictions': predictions,
                'individual_probabilities': probabilities,
                'confidence': np.max(avg_prob, axis=1)
            }
        else:
            # ä½¿ç”¨å•ä¸ªæœ€ä½³æ¨¡å‹
            best_model = list(predictions.keys())[0]
            results = {
                'prediction': predictions[best_model],
                'probability': probabilities[best_model],
                'model_used': best_model
            }
        
        return results
    
    return ensemble_predict

# åˆ›å»ºé¢„æµ‹å‡½æ•°
ensemble_predictor = create_ensemble_predictor()

# ================================
# é¢„æµ‹ç¤ºä¾‹å’ŒéªŒè¯
# ================================

print("\nğŸ”® é›†æˆæ¨¡å‹é¢„æµ‹ç¤ºä¾‹...")

# ä½¿ç”¨æµ‹è¯•é›†çš„å‡ ä¸ªæ ·æœ¬è¿›è¡Œæ¼”ç¤º
sample_indices = np.random.choice(len(X_test), min(3, len(X_test)), replace=False)

for i, idx in enumerate(sample_indices, 1):
    print(f"\næ ·æœ¬ {i} é¢„æµ‹ç»“æœ:")
    print("-" * 40)
    
    # å®é™…æ ‡ç­¾
    actual = le.classes_[y_test[idx]]
    print(f"ğŸ¯ å®é™…æ ‡ç­¾: {actual}")
    
    # å„æ¨¡å‹é¢„æµ‹ç»“æœ
    print(f"ğŸ¤– å„æ¨¡å‹é¢„æµ‹:")
    for model_name, result in model_results.items():
        predicted = le.classes_[result['y_pred'][idx]]
        probability = result['y_pred_proba'][idx]
        confidence = max(probability, 1-probability) if len(le.classes_) == 2 else max(result['model'].predict_proba(X_test[idx:idx+1])[0])
        status = "âœ…" if predicted == actual else "âŒ"
        print(f"  {model_name:<20}: {predicted} ({confidence:.3f}) {status}")

# ================================
# æœ€ç»ˆæ€»ç»“æŠ¥å‘Š
# ================================

print("\n" + "="*80)
print("ğŸ‰ å¢å¼ºç‰ˆå¾®ç”Ÿç‰©ç»„å­¦é¢„æµ‹ç³»ç»Ÿå®ŒæˆæŠ¥å‘Š")
print("="*80)

print(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
print(f"  â€¢ æ€»æ ·æœ¬æ•°: {df.shape[0]}")
print(f"  â€¢ åŸå§‹ç‰¹å¾æ•°: {df.shape[1]-1}")
print(f"  â€¢ ä½¿ç”¨ç‰¹å¾æ•°: {len(selected_feature_names)}")
print(f"  â€¢ ç›®æ ‡ç±»åˆ«: {len(le.classes_)} ç±»")

print(f"\nğŸ¤– è®­ç»ƒæ¨¡å‹æ•°é‡: {len(model_results)}")
traditional_count = sum(1 for name in model_results.keys() if 'Neural Network' not in name and 'Attention' not in name)
deep_learning_count = len(model_results) - traditional_count

print(f"  â€¢ ä¼ ç»Ÿæœºå™¨å­¦ä¹ : {traditional_count} ä¸ª")
if DEEP_LEARNING_AVAILABLE:
    print(f"  â€¢ æ·±åº¦å­¦ä¹ æ¨¡å‹: {deep_learning_count} ä¸ª")

print(f"\nğŸ† æ¨¡å‹æ€§èƒ½æ’å:")
for i, (name, result) in enumerate(sorted_models[:5], 1):
    print(f"  {i}. {name}: AUC={result['auc']:.3f}, å‡†ç¡®ç‡={result['accuracy']:.1%}")

print(f"\nğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:")
print(f"  â€¢ æ¨¡å‹æ–‡ä»¶: {len(model_results)} ä¸ª")
print(f"  â€¢ é¢„å¤„ç†å™¨: 3 ä¸ª")
print(f"  â€¢ ç‰¹å¾ä¿¡æ¯: 2 ä¸ª")
print(f"  â€¢ æ€§èƒ½æŠ¥å‘Š: 1 ä¸ª")

print(f"\nğŸš€ ç³»ç»ŸåŠŸèƒ½:")
print(f"  âœ… æ•°æ®é¢„å¤„ç†å’Œæ¸…æ´—")
print(f"  âœ… å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•")
if DEEP_LEARNING_AVAILABLE:
    print(f"  âœ… æ·±åº¦å­¦ä¹ å’Œæ³¨æ„åŠ›æœºåˆ¶")
print(f"  âœ… æ¨¡å‹é›†æˆå’ŒæŠ•ç¥¨")
print(f"  âœ… å¯è§†åŒ–åˆ†æ")
print(f"  âœ… é¢„æµ‹æ¥å£")

# æ€§èƒ½ç­‰çº§è¯„ä¼°
best_auc = sorted_models[0][1]['auc']
if best_auc > 0.95:
    grade = "ğŸŒŸğŸŒŸğŸŒŸ å“è¶Š"
elif best_auc > 0.9:
    grade = "ğŸŒŸğŸŒŸ ä¼˜ç§€"
elif best_auc > 0.8:
    grade = "ğŸŒŸ è‰¯å¥½"
elif best_auc > 0.7:
    grade = "ğŸ‘Œ å¯æ¥å—"
else:
    grade = "âš ï¸ éœ€è¦æ”¹è¿›"

print(f"\nğŸ–ï¸ ç³»ç»Ÿè¯„çº§: {grade}")
print(f"ğŸ“ˆ æ¨èä½¿ç”¨: {best_model_name}")

print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
if best_auc > 0.9:
    print(f"  â€¢ æ¨¡å‹æ€§èƒ½ä¼˜å¼‚ï¼Œå¯ç›´æ¥æŠ•å…¥ä½¿ç”¨")
    print(f"  â€¢ å»ºè®®ä½¿ç”¨é›†æˆé¢„æµ‹æé«˜ç¨³å®šæ€§")
elif best_auc > 0.8:
    print(f"  â€¢ æ¨¡å‹æ€§èƒ½è‰¯å¥½ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
    print(f"  â€¢ å¯è€ƒè™‘å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®")
else:
    print(f"  â€¢ å»ºè®®æ”¶é›†æ›´å¤šæ•°æ®æˆ–å°è¯•ç‰¹å¾å·¥ç¨‹")
    print(f"  â€¢ å¯è€ƒè™‘è°ƒæ•´æ¨¡å‹è¶…å‚æ•°")

print(f"\nğŸ“ æŠ€æœ¯æ”¯æŒ:")
print(f"  â€¢ é¢„æµ‹å‡½æ•°: ensemble_predictor()")
print(f"  â€¢ æ‰¹é‡é¢„æµ‹: æ”¯æŒCSVæ–‡ä»¶å’ŒDataFrame")
print(f"  â€¢ é›†æˆé¢„æµ‹: å¤šæ¨¡å‹æŠ•ç¥¨æœºåˆ¶")

print("\n" + "="*80)
print("æ„Ÿè°¢ä½¿ç”¨å¢å¼ºç‰ˆå¾®ç”Ÿç‰©ç»„å­¦é¢„æµ‹ç³»ç»Ÿ! ğŸ¦ ğŸ§ ğŸ”¬")
print("="*80)

# åˆ›å»ºä½¿ç”¨ç¤ºä¾‹
print(f"\nğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
print(f"""
# å•ä¸ªæ ·æœ¬é¢„æµ‹
results = ensemble_predictor(new_data_df=your_dataframe)
print(results['ensemble_prediction'])

# ä»æ–‡ä»¶é¢„æµ‹
results = ensemble_predictor(new_data_path='new_samples.csv')
print(results['ensemble_probability'])

# æŸ¥çœ‹ä¸ªåˆ«æ¨¡å‹ç»“æœ
print(results['individual_predictions'])
""")
